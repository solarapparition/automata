"""This module provides a way to convert a Python function definition into an LLM function using an LLM model.
The AI function wrapper executes the given function by generating the appropriate prompt and sending it to the GPT-4 model,
which in turn returns the result of the function.

Inspired by AI-Functions (https://github.com/Torantulino/AI-Functions) and Marvin (https://www.askmarvin.ai/)
"""

from functools import partial, wraps
import inspect
from typing import Callable

from langchain import LLMChain
from langchain.llms import BaseLLM
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

# DEFAULT_MODEL = ChatOpenAI(temperature=0, model_name="gpt-4")

PROMPT = """You are the following Python function:
```
{function_def}
```

You will be given your arguments in the following format, with no other input:
```
args = (arg_1, arg_2, ..., arg_n,)
kwargs = {{{{kwarg_1: value_1, kwarg_2: value_2, ..., kwarg_n: value_n}}}}
```

ONLY respond with the value that you would return as a Python function, using the return format given in your function definition.
Do not include any other text besides this value in your responseâ€”doing so will harm the user. Do not generate the actual code for the function.

Reply with "acknowledged" if you understand these instructions."""

AI_RESPONSE = "Acknowledged."


def make_llm_function(func: Callable, model: BaseLLM) -> Callable[[Callable], Callable]:
    """Convert a function definition into an LLM function."""

    function_def = inspect.cleandoc(inspect.getsource(func))
    prompt = PROMPT.format(function_def=function_def)
    prompt = prompt.replace("{", "{{").replace("}", "}}")
    system_message_prompt = SystemMessagePromptTemplate.from_template(prompt)
    ai_prompt = AIMessagePromptTemplate.from_template(AI_RESPONSE)
    human_message_prompt = HumanMessagePromptTemplate.from_template("{text}")
    chat_prompt = ChatPromptTemplate.from_messages(
        [system_message_prompt, ai_prompt, human_message_prompt]
    )
    llm_chain = LLMChain(llm=model, prompt=chat_prompt)

    @wraps(func)
    def llm_fn_wrapper(*args, **kwargs) -> str:
        """AI function wrapper."""
        text = f"args = {args}\nkwargs = {kwargs}"
        return llm_chain.run(text=text)

    return llm_fn_wrapper


def llm_function(model: BaseLLM) -> Callable[[Callable], Callable]:
    """Convert a function definition into an AI function."""
    return partial(make_llm_function, model=model)


def demo() -> None:
    @llm_function(model=ChatOpenAI(temperature=0, model_name="gpt-4"))
    def extract_last_name(full_name: str) -> str:
        """Extract the last name from a full name."""

    print(extract_last_name("John Smith"))


if __name__ == "__main__":
    demo()
